ğŸ§  LSTM Text Generator (Word-Level)

A word-level LSTM-based text generator trained on the Complete Works of Shakespeare from Project Gutenberg.
This script demonstrates natural language generation using a recurrent neural network in TensorFlow/Keras.

ğŸ“˜ Overview

This project builds a deep learning model that learns Shakespeareâ€™s writing style and generates similar text word by word.

The pipeline includes:

Dataset download (Shakespeareâ€™s works via Project Gutenberg)

Text preprocessing (cleaning, lowercasing, removing punctuation)

Tokenization and sequence creation (word-level)

Model training (Embedding â†’ LSTM â†’ Dense Softmax)

Text generation using a seed prompt

ğŸ§© Features

Automatic dataset download and preprocessing

Word-level LSTM training pipeline

Model checkpointing and early stopping

Adjustable sequence length, embedding size, and temperature for sampling

Command-line interface for training and generation

ğŸ§  Model Architecture
Embedding (vocab_size â†’ 128)
        â†“
LSTM (256 units)
        â†“
Dense (softmax output over vocabulary)

ğŸ§° Requirements

Install dependencies before running the project:

pip install tensorflow numpy requests tqdm

ğŸ“‚ Project Structure
lstm_text_generator/
â”‚
â”œâ”€â”€ lstm_text_generator.py   # Main script
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ shakespeare.txt      # Raw dataset (auto-downloaded)
â”‚   â”œâ”€â”€ tokenizer.json       # Saved tokenizer
â”‚   â””â”€â”€ lstm_text_gen.h5     # Trained model
â””â”€â”€ README.md                # Documentation

âš™ï¸ Usage
1. Train the model
python lstm_text_generator.py --train


Downloads and preprocesses the Shakespeare dataset

Tokenizes text into word sequences

Trains the LSTM model with early stopping and checkpointing

Saves the model and tokenizer to the data/ folder

2. Generate text

Once training is complete, generate text using a seed phrase:

python lstm_text_generator.py --generate --seed "to be or not to be" --length 50


Optional arguments:

--seed â†’ Starting text prompt

--length â†’ Number of words to generate

--temperature â†’ Controls creativity (default = 1.0).

Lower = safer / more predictable

Higher = more random / creative

ğŸ§ª Example Output
> python lstm_text_generator.py --generate --seed "love is" --length 20

love is not a man of war nor a friend but a poor heart that cannot speak for fear

ğŸ—ƒï¸ Saved Files
File	Description
data/shakespeare.txt	Raw dataset from Project Gutenberg
data/tokenizer.json	Tokenizer vocabulary used for encoding words
data/lstm_text_gen.h5	Trained model weights
ğŸš€ Tips for Better Results

Train longer (e.g., 100+ epochs) with a GPU for better fluency

Increase the dataset size (use more Shakespeare works or combine other authors)

Experiment with temperature and sequence length for varied outputs

ğŸ“„ License

This project uses public-domain text from Project Gutenberg
.
All generated outputs are free to use.
